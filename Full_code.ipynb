{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d83e336-b578-4319-b09e-a18cc1de07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import QED\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem  # For molecule validation\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "\n",
    "__special__ = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\"}\n",
    "# Checking for MPS availability\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca029052-408d-45cd-a3ad-1fbef25a3d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RDlogger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m one_hot\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mRDlogger\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __special__ \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<EOS>\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Checking for MPS availability\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'RDlogger'"
     ]
    }
   ],
   "source": [
    "def preprocess_smiles(input_file, output_file):\n",
    "    # Read the original SMILES file\n",
    "    smiles_data = pd.read_csv(input_file, usecols=[0])\n",
    "    \n",
    "    # Prepare lists to store valid SMILES, LogP, and QED\n",
    "    valid_smiles = []\n",
    "    logp_list = []\n",
    "    qed_list = []\n",
    "    total_smiles = len(smiles_data)  # Total number of SMILES in the input file\n",
    "\n",
    "    for sm in tqdm(smiles_data.iloc[:, 0].dropna().tolist(), desc=\"Processing SMILES\"):\n",
    "        mol = Chem.MolFromSmiles(sm)\n",
    "        if mol:  # Check if the molecule is valid\n",
    "            valid_smiles.append(sm)\n",
    "            logp = Descriptors.MolLogP(mol)\n",
    "            qed = QED.qed(mol)\n",
    "            logp_list.append(logp)\n",
    "            qed_list.append(qed)\n",
    "    \n",
    "    # Create a DataFrame with the valid SMILES and their properties\n",
    "    processed_data = pd.DataFrame({\n",
    "        'SMILES': valid_smiles,\n",
    "        'LogP': logp_list,\n",
    "        'QED': qed_list\n",
    "    })\n",
    "    \n",
    "    # Save the new data to a CSV file\n",
    "    processed_data.to_csv(output_file, index=False)\n",
    "    print(f\"Preprocessed data saved to {output_file}\")\n",
    "    \n",
    "    # Calculate and print the percentage of valid SMILES\n",
    "    valid_percentage = (len(valid_smiles) / total_smiles) * 100\n",
    "    print(f\"Percentage of valid SMILES: {valid_percentage:.2f}%\")\n",
    "\n",
    "# Example usage\n",
    "preprocess_smiles('smiles_CHEMBL_22', 'processed_smiles_data_kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e755aec5-0588-4060-948d-117128cc40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smiles_data(torch.utils.data.Dataset):\n",
    "    def __init__(self, file, total=130):\n",
    "        self.total = total\n",
    "        \n",
    "        # Load the preprocessed dataset with SMILES and target variables\n",
    "        data = pd.read_csv(file).iloc[:self.total]\n",
    "        \n",
    "        self.smiles = data['SMILES'].tolist()\n",
    "        self.logp_qed = data[['LogP', 'QED']].values.tolist()\n",
    "        \n",
    "        tokens = functools.reduce(lambda acc, s: acc.union(set(s)), self.smiles, set())\n",
    "        self.vocsize = len(tokens) + len(__special__)\n",
    "        self.index2token = dict(enumerate(tokens, start=3))\n",
    "        self.index2token.update(__special__)\n",
    "        self.token2index = {v: k for k, v in self.index2token.items()}\n",
    "        \n",
    "        self.ints = [torch.LongTensor([self.token2index[s] for s in line]) for line in tqdm(self.smiles, \"Preparing the dataset\")]\n",
    "\n",
    "    def decode(self, indexes):\n",
    "        return \"\".join([self.index2token[index] for index in indexes if index not in __special__])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        special_added = torch.cat((\n",
    "            torch.LongTensor([self.token2index['<BOS>']]),\n",
    "            self.ints[i],\n",
    "            torch.LongTensor([self.token2index['<EOS>']]),\n",
    "        ), dim=0)\n",
    "        \n",
    "        logp, qed = self.logp_qed[i]  # Get the LogP and QED values for the current index\n",
    "        return one_hot(special_added, self.vocsize).float(), special_added, torch.tensor([logp, qed], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    seq_len = max([item[1].size(0) for item in batch])\n",
    "    padded_batch = []\n",
    "    properties = []\n",
    "\n",
    "    for one_hot_seq, special_added, props in batch:\n",
    "        pad_length = seq_len - special_added.size(0)\n",
    "        padded_special_added = torch.cat([\n",
    "            special_added,\n",
    "            torch.LongTensor([0] * pad_length)\n",
    "        ])\n",
    "        padded_one_hot_seq = one_hot(padded_special_added, one_hot_seq.size(1)).float()\n",
    "        padded_batch.append((padded_one_hot_seq, padded_special_added))\n",
    "        properties.append(props)\n",
    "    \n",
    "    one_hot_tensors = torch.stack([item[0] for item in padded_batch])\n",
    "    special_added_tensors = torch.stack([item[1] for item in padded_batch])\n",
    "    properties = torch.stack(properties)\n",
    "    \n",
    "    return one_hot_tensors, special_added_tensors, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c2acfe-42b6-45f3-b6be-172de1765e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, latent_dim=20, hidden_size=64):\n",
    "        super(SmilesVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_lstm = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(latent_dim, int(hidden_size/2), batch_first=True)\n",
    "        self.fc_out = nn.Linear(int(hidden_size/2), vocab_size)\n",
    "        \n",
    "        # Predictor for LogP and QED (2 outputs)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)  # 2 outputs for LogP and QED\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        _, (hidden, _) = self.encoder_lstm(x)  # Learns hidden representation of the input vector x\n",
    "        mu = self.fc_mu(hidden[-1])  # Use hidden representation to determine the center of the latent distribution (mean)\n",
    "        logvar = self.fc_logvar(hidden[-1])  # Use hidden representation to determine the spread of the latent distribution (variance)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):  # To sample from the distribution and ensure differentiability, use the reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * (std + 1e-10)\n",
    "        return z  # z is the sampled latent vector\n",
    "    \n",
    "    def decode(self, z, seq_len=10, tokenizer=None, temp=1.0):\n",
    "        batch_size = z.size(0)\n",
    "        generated_seq = torch.full((batch_size, 1), tokenizer['<BOS>'], dtype=torch.long, device=z.device)\n",
    "        hidden_state = None\n",
    "\n",
    "        logits_list = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            z_rep = z.unsqueeze(1).repeat(1, generated_seq.size(1), 1)\n",
    "            output, hidden_state = self.decoder_lstm(z_rep, hidden_state)\n",
    "            logits = self.fc_out(output[:, -1, :])  # Get logits for the last time step\n",
    "            logits_list.append(logits)\n",
    "\n",
    "            next_token_probs = torch.softmax(logits / temp, dim=-1)\n",
    "            next_token = torch.multinomial(next_token_probs, 1)\n",
    "\n",
    "            generated_seq = torch.cat((generated_seq, next_token), dim=1)\n",
    "\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "\n",
    "        # Retrieve <EOS> and <PAD> token indices from tokenizer\n",
    "        eos_token = tokenizer['<EOS>']\n",
    "        pad_token = tokenizer['<PAD>']\n",
    "\n",
    "        # Stop generation after EOS token is encountered\n",
    "        mask = torch.cumsum((generated_seq == eos_token).float(), dim=1)\n",
    "        generated_seq = generated_seq.masked_fill(mask > 0, pad_token)\n",
    "\n",
    "        return logits, generated_seq\n",
    "\n",
    "    def forward(self, x, tokenizer, seq_len=10):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        logits, _ = self.decode(z, seq_len=seq_len, tokenizer=tokenizer)\n",
    "        \n",
    "        # Predict LogP and QED\n",
    "        predicted_properties = self.predictor(z)\n",
    "        \n",
    "        return logits, mu, logvar, predicted_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d1780c-c045-4190-a63d-63602d9d820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(reconstructed_seq, original_seq, mu, logvar, predicted_properties, true_properties, epoch, n_epochs):\n",
    "    # Flatten the tensors for CrossEntropyLoss\n",
    "    flat_reconstructed_seq = reconstructed_seq.view(-1, reconstructed_seq.size(-1))  # (batch_size * seq_len, vocab_size)\n",
    "    flat_original_seq = original_seq.view(-1)  # (batch_size * seq_len)\n",
    "    \n",
    "    # Define a mask for the padding sequences\n",
    "    pad_mask = original_seq != 0  \n",
    "    \n",
    "    # Ensure reconstruction loss uses float logits and long target\n",
    "    reconstruction_loss = nn.CrossEntropyLoss(reduction='none')(flat_reconstructed_seq, flat_original_seq.long())\n",
    "    \n",
    "    # Construct the loss without the padding mask\n",
    "    reconstruction_loss = reconstruction_loss[pad_mask.view(-1)].mean()\n",
    "    \n",
    "    # KL Divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Predictor loss (MSE Loss)\n",
    "    property_loss = nn.MSELoss()(predicted_properties, true_properties)\n",
    "\n",
    "    # Define beta for KL annealing\n",
    "    beta = min(1.0, epoch / (n_epochs * 0.4))  # KL annealing\n",
    "\n",
    "    # Construct the total loss\n",
    "    total_loss = reconstruction_loss + (beta * kl_divergence) + property_loss\n",
    "    \n",
    "    return total_loss, reconstruction_loss.item(), kl_divergence.item(), property_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dad71be-a451-4dde-9fb8-e07530caea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing the dataset: 100%|████████| 150000/150000 [00:01<00:00, 108892.20it/s]\n",
      "Training: 100%|█████████████████████████████| 4688/4688 [43:12<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 35 done, total loss: 19199.545831918716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [43:43<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 35 done, total loss: 18955.75008201599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:02<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 35 done, total loss: 18920.805114507675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:16<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of 35 done, total loss: 18887.348878622055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:03<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 of 35 done, total loss: 18848.22046661377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:14<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 of 35 done, total loss: 18844.658971071243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:14<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 of 35 done, total loss: 18825.825362682343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:16<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 of 35 done, total loss: 18812.84031009674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:17<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 of 35 done, total loss: 18811.420698165894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:25<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 of 35 done, total loss: 18797.347196102142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:19<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 of 35 done, total loss: 18792.964566469193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:22<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 of 35 done, total loss: 18789.343333244324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:23<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 of 35 done, total loss: 18778.598415374756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:40<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 of 35 done, total loss: 18771.429621696472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:25<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 of 35 done, total loss: 18771.548410654068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:30<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 of 35 done, total loss: 18766.071352481842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:24<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 of 35 done, total loss: 18764.727172374725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:48<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 of 35 done, total loss: 18762.286507606506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:31<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 of 35 done, total loss: 18760.27905869484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:05<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 of 35 done, total loss: 18755.328515291214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:16<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 of 35 done, total loss: 18756.50675201416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:16<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 of 35 done, total loss: 18754.782150506973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:19<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 of 35 done, total loss: 18750.876132011414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:22<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 of 35 done, total loss: 18750.79875421524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:21<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 of 35 done, total loss: 18750.92317700386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:28<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 of 35 done, total loss: 18744.015348672867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:19<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 of 35 done, total loss: 18750.7018866539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:29<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 of 35 done, total loss: 18748.205770730972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:35<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 of 35 done, total loss: 18746.55018043518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:08<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 of 35 done, total loss: 18742.871630191803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 of 35 done, total loss: 18742.318988084793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:34<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 of 35 done, total loss: 18741.695187568665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:25<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 of 35 done, total loss: 18743.66960167885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:47<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 of 35 done, total loss: 18740.371859312057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████| 4688/4688 [44:54<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 of 35 done, total loss: 18740.7305932045\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses saved to vae_training_losses.csv and vae_training_losses.png\n",
      "VAE Training Complete! Model saved as vae_pred_subset150000_latent60_hidden256_epochs35.pt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_and_save_losses(epoch_reconstruction_losses, epoch_kl_divergences, epoch_property_losses, n_epochs):\n",
    "    \"\"\"\n",
    "    Plots and saves the reconstruction loss, KL divergence, and property loss across epochs.\n",
    "    \"\"\"\n",
    "    epochs = list(range(1, n_epochs + 1))\n",
    "    \n",
    "    # Plot Reconstruction Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, epoch_reconstruction_losses, label=\"Reconstruction Loss\")\n",
    "    plt.plot(epochs, epoch_kl_divergences, label=\"KL Divergence\")\n",
    "    plt.plot(epochs, epoch_property_losses, label=\"Property Loss (LogP/QED)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Losses Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f\"figs/vae_training_losses{n_epochs}.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save the losses to a CSV file\n",
    "    df_losses = pd.DataFrame({\n",
    "        \"Epoch\": epochs,\n",
    "        \"Reconstruction Loss\": epoch_reconstruction_losses,\n",
    "        \"KL Divergence\": epoch_kl_divergences,\n",
    "        \"Property Loss\": epoch_property_losses\n",
    "    })\n",
    "    df_losses.to_csv(f\"csv_logs/vae_training_losses{n_epochs}.csv\", index=False)\n",
    "    print(\"Training losses saved to vae_training_losses.csv and vae_training_losses.png\")\n",
    "\n",
    "def train_vae(train_file, batch_size=32, learning_rate=0.001, n_epochs=50, latent_dim=40, hidden_size=128, subset_size=50):\n",
    "    save_file = f\"vae_pred_subset{subset_size}_latent{latent_dim}_hidden{hidden_size}_epochs{n_epochs}.pt\"\n",
    "    \n",
    "    dataset = Smiles_data(train_file, total=subset_size)\n",
    "    \n",
    "    model = SmilesVAE(dataset.vocsize, latent_dim, hidden_size).to(device)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_reconstruction_losses = []\n",
    "    epoch_kl_divergences = []\n",
    "    epoch_property_losses = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        total_loss = 0.\n",
    "        total_reconstruction_loss = 0.\n",
    "        total_kl_divergence = 0.\n",
    "        total_property_loss = 0.\n",
    "\n",
    "        for iteration, (batch, target, true_properties) in enumerate(tqdm(dataloader, 'Training')):\n",
    "            batch, target, true_properties = batch.to(device), target.to(device), true_properties.to(device)\n",
    "            model.train()\n",
    "            seq_len = target.size(1)  # Get the sequence length from the target sequence\n",
    "            reconstructed_seq, mu, logvar, predicted_properties = model(batch, dataset.token2index, seq_len=seq_len)\n",
    "            \n",
    "            loss, reconstruction_loss, kl_divergence, property_loss = vae_loss_function(\n",
    "                reconstructed_seq, target, mu, logvar, predicted_properties, true_properties, epoch, n_epochs\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_reconstruction_loss += reconstruction_loss\n",
    "            total_kl_divergence += kl_divergence\n",
    "            total_property_loss += property_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_reconstruction_losses.append(total_reconstruction_loss / len(dataloader))\n",
    "        epoch_kl_divergences.append(total_kl_divergence / len(dataloader))\n",
    "        epoch_property_losses.append(total_property_loss / len(dataloader))\n",
    "\n",
    "        print(f\"Epoch {epoch} of {n_epochs} done, total loss: {total_loss}\")\n",
    "\n",
    "    # Plot and save the losses\n",
    "    plot_and_save_losses(epoch_reconstruction_losses, epoch_kl_divergences, epoch_property_losses, n_epochs)\n",
    "\n",
    "    torch.save({'tokenizer': dataset.index2token, 'model': model.cpu()}, f\"models/{save_file}\")\n",
    "    print(f\"VAE Training Complete! Model saved as {save_file}\")\n",
    "\n",
    "# Example call to the modified training function\n",
    "subset_size = 150000\n",
    "train_vae(train_file='processed_smiles_22.csv', batch_size=32, n_epochs=35, latent_dim=60, hidden_size=256, subset_size=subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14223728-e590-4de7-b441-b7d56158a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing temperature to 0.556 for more diversity\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.55\u001b[39m\n\u001b[1;32m    109\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m28\u001b[39m\n\u001b[0;32m--> 110\u001b[0m generated_smiles \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_molecules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_logp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_logp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_qed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_qed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Save generated SMILES to a file\u001b[39;00m\n\u001b[1;32m    113\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(generated_smiles, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMILES\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[24], line 54\u001b[0m, in \u001b[0;36mgenerate_molecules\u001b[0;34m(model, tokenizer, num_samples, seq_len, target_logp, target_qed, temp, max_temp, max_attempts)\u001b[0m\n\u001b[1;32m     51\u001b[0m     z \u001b[38;5;241m=\u001b[39m aim_for_target_properties(model, z, target_logp, target_qed)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Decode the latent vector to generate logits and the token sequence\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m logits, sampled_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken2index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass token2index\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Convert token indices back to SMILES string\u001b[39;00m\n\u001b[1;32m     57\u001b[0m token_indices \u001b[38;5;241m=\u001b[39m sampled_tokens[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Take the first (and only) sample from batch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 101\u001b[0m, in \u001b[0;36mSmilesVAE.decode\u001b[0;34m(self, z, seq_len, tokenizer, temp)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[1;32m    100\u001b[0m     z_rep \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, generated_seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     output, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Get logits for the last time step\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     logits_list\u001b[38;5;241m.\u001b[39mappend(logits)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-nightly/lib/python3.8/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to decode token indices into SMILES\n",
    "def decode_from_indices(token_indices, tokenizer):\n",
    "    # Join tokens into a string and remove any unwanted special tokens\n",
    "    smiles = \"\".join([tokenizer.get(i, '') for i in token_indices if i not in __special__])\n",
    "    \n",
    "    # Remove any unwanted characters like tabs or newlines from the generated string\n",
    "    smiles = smiles.replace('\\t', '').replace('\\n', '').strip()\n",
    "    \n",
    "    return smiles\n",
    "\n",
    "def validate_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            return True\n",
    "    except:\n",
    "        print(f\"SMILES parsing failed: {smiles}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_molecules(model, tokenizer, num_samples, seq_len, target_logp=None, target_qed=None, temp=1.0, max_temp=1.05, max_attempts=500):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    generated_smiles = []\n",
    "    unique_smiles = set()  # Set to track unique SMILES\n",
    "    token2index = {v: k for k, v in tokenizer.items()}  # Reverse tokenizer mapping\n",
    "    \n",
    "    try:\n",
    "        eos_token_index = token2index['<EOS>']  # Retrieve the index for the <EOS> token\n",
    "    except KeyError:\n",
    "        raise KeyError(\"The '<EOS>' token is missing in the tokenizer.\")\n",
    "    \n",
    "    initial_temp = temp  \n",
    "    sample_idx = 0\n",
    "    attempts = 0  \n",
    "    \n",
    "    while sample_idx < num_samples:\n",
    "        if attempts >= max_attempts:  \n",
    "            new_temp = min(temp * 1.01, max_temp)  # Increase temperature but cap it\n",
    "            if new_temp != temp:\n",
    "                temp = new_temp\n",
    "                print(f\"Increasing temperature to {round(temp,3)} for more diversity\")\n",
    "            attempts = 0 \n",
    "        \n",
    "        # Sample from the prior (standard normal) with applied temperature scaling\n",
    "        z = torch.randn(1, model.latent_dim) * temp  # Shape: (1, latent_dim)\n",
    "\n",
    "        # Check if we have target values to aim for\n",
    "        if target_logp is not None and target_qed is not None:\n",
    "            # Iteratively adjust z to aim for the target properties (e.g., through gradient descent)\n",
    "            z = aim_for_target_properties(model, z, target_logp, target_qed)\n",
    "        \n",
    "        # Decode the latent vector to generate logits and the token sequence\n",
    "        logits, sampled_tokens = model.decode(z, seq_len=seq_len, tokenizer=token2index, temp=temp)  # Pass token2index\n",
    "        \n",
    "        # Convert token indices back to SMILES string\n",
    "        token_indices = sampled_tokens[0].tolist()  # Take the first (and only) sample from batch\n",
    "        if eos_token_index in token_indices:\n",
    "            token_indices = token_indices[:token_indices.index(eos_token_index)]  # Stop at <EOS>\n",
    "        \n",
    "        smiles = decode_from_indices(token_indices, tokenizer)\n",
    "        \n",
    "        # Validate SMILES string\n",
    "        if validate_smiles(smiles):\n",
    "            if smiles not in unique_smiles:\n",
    "                print(f\"Valid unique SMILES: {smiles}\")\n",
    "                generated_smiles.append(smiles)\n",
    "                unique_smiles.add(smiles)\n",
    "                sample_idx += 1\n",
    "                attempts = 0  # Reset attempts counter after successful generation\n",
    "                temp = initial_temp  # Reset temperature to initial value\n",
    "            else:\n",
    "                attempts += 1  # Increment attempts\n",
    "        else:\n",
    "            attempts += 1  # Increment attempts for invalid SMILES\n",
    "    \n",
    "    return generated_smiles\n",
    "\n",
    "def aim_for_target_properties(model, z, target_logp, target_qed, lr=0.01, steps=10):\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([z], lr=lr)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_properties = model.predictor(z)\n",
    "        predicted_logp, predicted_qed = predicted_properties[0]\n",
    "\n",
    "        # Loss to guide z towards the target properties\n",
    "        loss = ((predicted_logp - target_logp) ** 2 + (predicted_qed - target_qed) ** 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return z.detach()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model and tokenizer\n",
    "    checkpoint = torch.load('models/vae_pred_subset150000_latent100_hidden256_epochs8.pt')\n",
    "    model = checkpoint['model']\n",
    "    tokenizer = checkpoint['tokenizer']\n",
    "    \n",
    "    # Define target properties\n",
    "    target_logp = 1.8\n",
    "    target_qed = 0.6\n",
    "    \n",
    "    # Generate molecules\n",
    "    num_samples = 11\n",
    "    temperature = 0.55\n",
    "    seq_len = 28\n",
    "    generated_smiles = generate_molecules(model, tokenizer, num_samples, seq_len, target_logp=target_logp, target_qed=target_qed, temp=temperature)\n",
    "\n",
    "    # Save generated SMILES to a file\n",
    "    df = pd.DataFrame(generated_smiles, columns=[\"SMILES\"])\n",
    "    df.to_csv(f\"molecules/generated_molecules_len{seq_len}.csv\", index=False)\n",
    "    \n",
    "    print(f\"Generated {len(generated_smiles)} valid molecules and saved to generated_molecules.csv\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdf565c8-295d-487e-8cf6-b5450e6f2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating seq_len = 3\n",
      "Valid unique SMILES: CC\n",
      "Valid unique SMILES: OC\n",
      "Valid unique SMILES: CO\n",
      "Valid unique SMILES: ON\n",
      "Valid unique SMILES: CN\n",
      "Valid unique SMILES: NO\n",
      "Valid unique SMILES: NC\n",
      "Valid unique SMILES: FC\n",
      "Valid unique SMILES: CS\n",
      "Valid unique SMILES: OO\n",
      "Sampling efficiency for seq_len 3: 22.73%\n",
      "Evaluating seq_len = 4\n",
      "Valid unique SMILES: CCC\n",
      "Valid unique SMILES: OCN\n",
      "Valid unique SMILES: COC\n",
      "Valid unique SMILES: OSO\n",
      "Valid unique SMILES: CNC\n",
      "Valid unique SMILES: ClC\n",
      "Valid unique SMILES: FNC\n",
      "Valid unique SMILES: SCC\n",
      "Valid unique SMILES: CNN\n",
      "Valid unique SMILES: CON\n",
      "Sampling efficiency for seq_len 4: 16.13%\n",
      "Evaluating seq_len = 5\n",
      "Valid unique SMILES: OSCC\n",
      "Valid unique SMILES: CCCC\n",
      "Valid unique SMILES: COCC\n",
      "Valid unique SMILES: ClCC\n",
      "Valid unique SMILES: ONCC\n",
      "Valid unique SMILES: CCCN\n",
      "Valid unique SMILES: CNOP\n",
      "Valid unique SMILES: NCCC\n",
      "Valid unique SMILES: NNCC\n",
      "Valid unique SMILES: CNCC\n",
      "Sampling efficiency for seq_len 5: 7.75%\n",
      "Evaluating seq_len = 6\n",
      "Valid unique SMILES: CCC=C\n",
      "Valid unique SMILES: CCCCO\n",
      "Valid unique SMILES: CONCN\n",
      "Valid unique SMILES: NN(C)\n",
      "Valid unique SMILES: CCCCC\n",
      "Valid unique SMILES: CC(C)\n",
      "Valid unique SMILES: COCCO\n",
      "Valid unique SMILES: ONN=C\n",
      "Valid unique SMILES: CO(C)\n",
      "Valid unique SMILES: NC(C)\n",
      "Sampling efficiency for seq_len 6: 3.15%\n",
      "Evaluating seq_len = 7\n",
      "Valid unique SMILES: CNCCNN\n",
      "Valid unique SMILES: CCOCNN\n",
      "Valid unique SMILES: CC(CC)\n",
      "Valid unique SMILES: OC1cc1\n",
      "Valid unique SMILES: CCCCNC\n",
      "Valid unique SMILES: OC(CO)\n",
      "Valid unique SMILES: CCCCOC\n",
      "Valid unique SMILES: COCCOO\n",
      "Valid unique SMILES: CC1CN1\n",
      "Valid unique SMILES: CC(C)C\n",
      "Sampling efficiency for seq_len 7: 1.31%\n",
      "Evaluating seq_len = 8\n",
      "Valid unique SMILES: CN1cc1C\n",
      "Valid unique SMILES: CC(C)=C\n",
      "Valid unique SMILES: CCC1Oc1\n",
      "Valid unique SMILES: CC1CcC1\n",
      "Valid unique SMILES: NC1=cC1\n",
      "Valid unique SMILES: OC1Ccc1\n",
      "Valid unique SMILES: COc1NC1\n",
      "Valid unique SMILES: CCN(CC)\n",
      "Valid unique SMILES: CSCCOOO\n",
      "Valid unique SMILES: CCC1CO1\n",
      "Sampling efficiency for seq_len 8: 0.84%\n",
      "Evaluating seq_len = 9\n",
      "Valid unique SMILES: CC1COcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CNc1ccc1\n",
      "Valid unique SMILES: CCCC1nN1\n",
      "Valid unique SMILES: CC1=ccN1\n",
      "Valid unique SMILES: CNC1cOc1\n",
      "Valid unique SMILES: OOC1Ccc1\n",
      "Valid unique SMILES: C(CNO)CC\n",
      "Valid unique SMILES: FOCCCCCS\n",
      "Valid unique SMILES: OCCCONCO\n",
      "Valid unique SMILES: CN1COsN1\n",
      "Sampling efficiency for seq_len 9: 0.44%\n",
      "Evaluating seq_len = 10\n",
      "Valid unique SMILES: CC(n1)CC1\n",
      "Valid unique SMILES: CC1cCCcC1\n",
      "Valid unique SMILES: COCCCCN\\C\n",
      "Valid unique SMILES: OC1NcOOc1\n",
      "Valid unique SMILES: OOc1NCOC1\n",
      "Valid unique SMILES: OCC[O]CNC\n",
      "Valid unique SMILES: CC(C)OCCC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC(COCC)O\n",
      "Valid unique SMILES: CC(cc1)N1\n",
      "Valid unique SMILES: CC(C)NCCN\n",
      "Sampling efficiency for seq_len 10: 0.54%\n",
      "Evaluating seq_len = 11\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC(CCl)ONC\n",
      "Valid unique SMILES: NC1CccCNc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: COC1CcCcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1COCOcc1\n",
      "Valid unique SMILES: CCC1CcCCc1\n",
      "Valid unique SMILES: CS1=cc1CCN\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCC1ccCC1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: OCC1COc1OC\n",
      "Valid unique SMILES: CCc1OOOCC1\n",
      "Valid unique SMILES: CNc1CcCN1C\n",
      "Sampling efficiency for seq_len 11: 0.20%\n",
      "Evaluating seq_len = 12\n",
      "Valid unique SMILES: NC1c=c2cc21\n",
      "Valid unique SMILES: OC1CcOOCNC1\n",
      "Valid unique SMILES: On1CNccCC1N\n",
      "Valid unique SMILES: NC1=nOCNcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: COCc1Cc1C=C\n",
      "Valid unique SMILES: FOc1CCc1CSC\n",
      "Valid unique SMILES: COC1Cc=ncc1\n",
      "Valid unique SMILES: C\\C(C)NCOCO\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1CnC1(CO)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1COCBcOC1\n",
      "Sampling efficiency for seq_len 12: 0.24%\n",
      "Evaluating seq_len = 13\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CC1ncccSC1=O\n",
      "Valid unique SMILES: CNc1cNCC=CC1\n",
      "Valid unique SMILES: O\\1\\CCCcOnc1\n",
      "Valid unique SMILES: OC1CCccCCcc1\n",
      "Valid unique SMILES: CN1CcCnCcNO1\n",
      "Valid unique SMILES: CCCCOOc1CCc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCCCC1O(cC1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CNC1cCc(cN1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1ccCC2Cc21\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CN1NCc(NN1)C\n",
      "Sampling efficiency for seq_len 13: 0.20%\n",
      "Evaluating seq_len = 14\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CC1CCc/Cc1CCS\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CCc1csccC1C=C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCc1cOnCcCcc1\n",
      "Valid unique SMILES: C\\c1c(cOc1)CC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: OCC=COON(N)NC\n",
      "Valid unique SMILES: CCc1NCccccCc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CN(Cc1cCC1CC)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1=cCC2CNc21\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: OC1=cnccOOcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Valid unique SMILES: FNCCNC1CCC1=C\n",
      "Sampling efficiency for seq_len 14: 0.05%\n",
      "Evaluating seq_len = 15\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Valid unique SMILES: CN(COc1cccNC1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCCC1COCcNSC1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCCCOc1CcCCcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CNCC2c/C2N(O)N\n",
      "Valid unique SMILES: CC1ccccCc1CNCO\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CSc1cc=ccCNOc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: OOc1ccccONc1CN\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Valid unique SMILES: CCc1ccccCNNc1C\n",
      "Valid unique SMILES: OOCCCCn2CCn2OC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: NC1CccC#CcCNc1\n",
      "Sampling efficiency for seq_len 15: 0.05%\n",
      "Evaluating seq_len = 16\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: FC(nOC2)\\nc2CCC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: NN1C=C/cCc2SC12\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1CCOcCSccCc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1ccCCC=c(C1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CC1CcCcCCCCNcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Valid unique SMILES: CN1nCCCCN1C2OC2\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CN(s=CcCc2)cCC2\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCN1cccnC=CCc1O\n",
      "Valid unique SMILES: Cc1cCCcNCcccc1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1CccCS1N(CC)\n",
      "Sampling efficiency for seq_len 16: 0.02%\n",
      "Evaluating seq_len = 17\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Valid unique SMILES: CC1COcCCCC11CN1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: C1CCCccCc1(NC=C)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: Cc(CcC(CC1))1CNC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: OCN1N(c1C2CC)cn2\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CC(=NC(CNNN)CSC)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCCCc2CCCc1CC21N\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: COC(S1ccCCcc1CC)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCCCN1cNCCcc=c1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1cC12ON(OCc2)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCc1CcOCccSccCN1\n",
      "Sampling efficiency for seq_len 17: 0.02%\n",
      "Saved sampling efficiency results to sampling_efficiency_per_seq_len.csv\n",
      "    seq_len  sampling_efficiency\n",
      "0         3            22.727273\n",
      "1         4            16.129032\n",
      "2         5             7.751938\n",
      "3         6             3.154574\n",
      "4         7             1.310616\n",
      "5         8             0.841751\n",
      "6         9             0.443459\n",
      "7        10             0.544959\n",
      "8        11             0.204625\n",
      "9        12             0.242954\n",
      "10       13             0.199322\n",
      "11       14             0.048022\n",
      "12       15             0.048077\n",
      "13       16             0.020873\n",
      "14       17             0.016955\n"
     ]
    }
   ],
   "source": [
    "def compute_sampling_efficiency(generated_smiles, total_attempts):\n",
    "    \"\"\"\n",
    "    Computes the sampling efficiency of the model.\n",
    "\n",
    "    Sampling efficiency is defined as the ratio of valid unique SMILES strings\n",
    "    to the total number of attempts made during the sampling process.\n",
    "\n",
    "    Args:\n",
    "        generated_smiles (list): A list of valid unique SMILES strings.\n",
    "        total_attempts (int): Total number of sampling attempts.\n",
    "\n",
    "    Returns:\n",
    "        float: The sampling efficiency as a percentage.\n",
    "    \"\"\"\n",
    "    num_unique_smiles = len(generated_smiles)\n",
    "    if total_attempts == 0:\n",
    "        return 0.0\n",
    "    return (num_unique_smiles / total_attempts) * 100\n",
    "\n",
    "\n",
    "def generate_molecules_with_efficiency(model, tokenizer, num_samples, seq_len, target_logp=None, target_qed=None, temp=0.55, max_temp=1.05, max_attempts=500):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    generated_smiles = []\n",
    "    unique_smiles = set()  # Set to track unique SMILES\n",
    "    token2index = {v: k for k, v in tokenizer.items()}  # Reverse tokenizer mapping\n",
    "    total_attempts = 0  # Track the total number of attempts\n",
    "\n",
    "    try:\n",
    "        eos_token_index = token2index['<EOS>']  # Retrieve the index for the <EOS> token\n",
    "    except KeyError:\n",
    "        raise KeyError(\"The '<EOS>' token is missing in the tokenizer.\")\n",
    "    \n",
    "    initial_temp = temp  \n",
    "    sample_idx = 0\n",
    "    attempts = 0  \n",
    "    \n",
    "    while sample_idx < num_samples:\n",
    "        if attempts >= max_attempts:  \n",
    "            new_temp = min(temp * 1.01, max_temp)  # Increase temperature but cap it\n",
    "            if new_temp != temp:\n",
    "                temp = new_temp\n",
    "                print(f\"Increasing temperature to {round(temp,3)} for more diversity\")\n",
    "            attempts = 0 \n",
    "        \n",
    "        # Sample from the prior (standard normal) with applied temperature scaling\n",
    "        z = torch.randn(1, model.latent_dim) * temp  # Shape: (1, latent_dim)\n",
    "\n",
    "        # Check if we have target values to aim for\n",
    "        if target_logp is not None and target_qed is not None:\n",
    "            # Iteratively adjust z to aim for the target properties (e.g., through gradient descent)\n",
    "            z = aim_for_target_properties(model, z, target_logp, target_qed)\n",
    "        \n",
    "        # Decode the latent vector to generate logits and the token sequence\n",
    "        logits, sampled_tokens = model.decode(z, seq_len=seq_len, tokenizer=token2index, temp=temp)  # Pass token2index\n",
    "        \n",
    "        # Convert token indices back to SMILES string\n",
    "        token_indices = sampled_tokens[0].tolist()  # Take the first (and only) sample from batch\n",
    "        if eos_token_index in token_indices:\n",
    "            token_indices = token_indices[:token_indices.index(eos_token_index)]  # Stop at <EOS>\n",
    "        \n",
    "        smiles = decode_from_indices(token_indices, tokenizer)\n",
    "        \n",
    "        # Validate SMILES string\n",
    "        if validate_smiles(smiles):\n",
    "            if smiles not in unique_smiles:\n",
    "                print(f\"Valid unique SMILES: {smiles}\")\n",
    "                generated_smiles.append(smiles)\n",
    "                unique_smiles.add(smiles)\n",
    "                sample_idx += 1\n",
    "                attempts = 0  # Reset attempts counter after successful generation\n",
    "                temp = initial_temp  # Reset temperature to initial value\n",
    "            else:\n",
    "                attempts += 1  # Increment attempts\n",
    "        else:\n",
    "            attempts += 1  # Increment attempts for invalid SMILES\n",
    "        \n",
    "        total_attempts += 1  # Count every attempt\n",
    "    \n",
    "    sampling_efficiency = compute_sampling_efficiency(generated_smiles, total_attempts)\n",
    "    # Save generated SMILES to a file\n",
    "    df = pd.DataFrame(generated_smiles, columns=[\"SMILES\"])\n",
    "    df.to_csv(f\"molecules/generated_molecules_len{seq_len}.csv\", index=False)\n",
    "    print(f\"Sampling efficiency for seq_len {seq_len}: {sampling_efficiency:.2f}%\")\n",
    "\n",
    "    return generated_smiles, sampling_efficiency\n",
    "\n",
    "\n",
    "def evaluate_sampling_efficiency_across_seq_lengths(model, tokenizer, num_samples_per_seq_len, seq_len_range, target_logp=None, target_qed=None, temp=1.0):\n",
    "    \"\"\"\n",
    "    Evaluates sampling efficiency across a range of sequence lengths and saves the results.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to generate molecules.\n",
    "        tokenizer: Tokenizer for converting indices to SMILES strings.\n",
    "        num_samples_per_seq_len (int): Number of samples to generate per sequence length.\n",
    "        seq_len_range (list): A list of sequence lengths to evaluate.\n",
    "        target_logp: Optional target logP value for property-based generation.\n",
    "        target_qed: Optional target QED value for property-based generation.\n",
    "        temp (float): Initial temperature for sampling.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing seq_len and sampling efficiency values.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for seq_len in seq_len_range:\n",
    "        print(f\"Evaluating seq_len = {seq_len}\")\n",
    "        _, sampling_efficiency = generate_molecules_with_efficiency(\n",
    "            model, tokenizer, num_samples_per_seq_len, seq_len, target_logp=target_logp, target_qed=target_qed, temp=temp\n",
    "        )\n",
    "        results.append({'seq_len': seq_len, 'sampling_efficiency': sampling_efficiency})\n",
    "    \n",
    "    # Convert results to DataFrame for easy saving\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"csv_logs/sampling_efficiency_per_seq_len.csv\", index=False)\n",
    "    print(\"Saved sampling efficiency results to sampling_efficiency_per_seq_len.csv\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model and tokenizer\n",
    "    checkpoint = torch.load('models/vae_pred_subset150000_latent100_hidden256_epochs8.pt')\n",
    "    model = checkpoint['model']\n",
    "    tokenizer = checkpoint['tokenizer']\n",
    "    \n",
    "    # Define target properties (optional)\n",
    "    target_logp = 2.0\n",
    "    target_qed = 0.6\n",
    "    \n",
    "    # Evaluate sampling efficiency across a range of sequence lengths\n",
    "    seq_len_range = range(3, 18, 1)  # Sequence lengths from 10 to 50, in steps of 5\n",
    "    num_samples_per_seq_len = 10  # Generate 10 samples for each sequence length\n",
    "    \n",
    "    df_efficiency = evaluate_sampling_efficiency_across_seq_lengths(\n",
    "        model, tokenizer, num_samples_per_seq_len, seq_len_range, target_logp=target_logp, target_qed=target_qed\n",
    "    )\n",
    "\n",
    "    print(df_efficiency)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
