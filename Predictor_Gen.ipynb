{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d83e336-b578-4319-b09e-a18cc1de07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import QED\n",
    "from rdkit import RDLogger\n",
    "from rdkit import Chem  # For molecule validation\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "RDLogger.DisableLog('rdApp.*') \n",
    "\n",
    "\n",
    "__special__ = {0: \"<PAD>\", 1: \"<BOS>\", 2: \"<EOS>\"}\n",
    "# Checking for MPS availability\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca029052-408d-45cd-a3ad-1fbef25a3d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SMILES:   0%|                 | 1923/542672 [00:03<14:13, 633.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentage of valid SMILES: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_percentage\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mpreprocess_smiles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmiles_CHEMBL_22\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_smiles_data_kaggle.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mpreprocess_smiles\u001b[0;34m(input_file, output_file)\u001b[0m\n\u001b[1;32m     14\u001b[0m valid_smiles\u001b[38;5;241m.\u001b[39mappend(sm)\n\u001b[1;32m     15\u001b[0m logp \u001b[38;5;241m=\u001b[39m Descriptors\u001b[38;5;241m.\u001b[39mMolLogP(mol)\n\u001b[0;32m---> 16\u001b[0m qed \u001b[38;5;241m=\u001b[39m \u001b[43mQED\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m logp_list\u001b[38;5;241m.\u001b[39mappend(logp)\n\u001b[1;32m     18\u001b[0m qed_list\u001b[38;5;241m.\u001b[39mappend(qed)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-nightly/lib/python3.8/site-packages/rdkit/Chem/QED.py:203\u001b[0m, in \u001b[0;36mqed\u001b[0;34m(mol, w, qedProperties)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Calculate the weighted sum of ADS mapped properties\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03msome examples from the QED paper, reference values from Peter G's original implementation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m0.234...\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qedProperties \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m   qedProperties \u001b[38;5;241m=\u001b[39m \u001b[43mproperties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m d \u001b[38;5;241m=\u001b[39m [ads(pi, adsParameters[name]) \u001b[38;5;28;01mfor\u001b[39;00m name, pi \u001b[38;5;129;01min\u001b[39;00m qedProperties\u001b[38;5;241m.\u001b[39m_asdict()\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    205\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(wi \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(di) \u001b[38;5;28;01mfor\u001b[39;00m wi, di \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(w, d))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-nightly/lib/python3.8/site-packages/rdkit/Chem/QED.py:176\u001b[0m, in \u001b[0;36mproperties\u001b[0;34m(mol)\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou need to provide a mol argument.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m mol \u001b[38;5;241m=\u001b[39m Chem\u001b[38;5;241m.\u001b[39mRemoveHs(mol)\n\u001b[1;32m    166\u001b[0m qedProperties \u001b[38;5;241m=\u001b[39m QEDproperties(\n\u001b[1;32m    167\u001b[0m   MW\u001b[38;5;241m=\u001b[39mrdmd\u001b[38;5;241m.\u001b[39m_CalcMolWt(mol),\n\u001b[1;32m    168\u001b[0m   ALOGP\u001b[38;5;241m=\u001b[39mCrippen\u001b[38;5;241m.\u001b[39mMolLogP(mol),\n\u001b[1;32m    169\u001b[0m   HBA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mlen\u001b[39m(mol\u001b[38;5;241m.\u001b[39mGetSubstructMatches(pattern)) \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m Acceptors\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mol\u001b[38;5;241m.\u001b[39mHasSubstructMatch(pattern)),\n\u001b[1;32m    172\u001b[0m   HBD\u001b[38;5;241m=\u001b[39mrdmd\u001b[38;5;241m.\u001b[39mCalcNumHBD(mol),\n\u001b[1;32m    173\u001b[0m   PSA\u001b[38;5;241m=\u001b[39mMolSurf\u001b[38;5;241m.\u001b[39mTPSA(mol),\n\u001b[1;32m    174\u001b[0m   ROTB\u001b[38;5;241m=\u001b[39mrdmd\u001b[38;5;241m.\u001b[39mCalcNumRotatableBonds(mol, rdmd\u001b[38;5;241m.\u001b[39mNumRotatableBondsOptions\u001b[38;5;241m.\u001b[39mStrict),\n\u001b[1;32m    175\u001b[0m   AROM\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(Chem\u001b[38;5;241m.\u001b[39mGetSSSR(Chem\u001b[38;5;241m.\u001b[39mDeleteSubstructs(Chem\u001b[38;5;241m.\u001b[39mMol(mol), AliphaticRings))),\n\u001b[0;32m--> 176\u001b[0m   ALERTS\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malert\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mStructuralAlerts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHasSubstructMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43malert\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# The replacement\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# AROM=Lipinski.NumAromaticRings(mol),\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# is not identical. The expression above tends to count more rings\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# N1C2=CC=CC=C2SC3=C1C=CC4=C3C=CC=C4\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# OC1=C(O)C=C2C(=C1)OC3=CC(=O)C(=CC3=C2C4=CC=CC=C4)O\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# CC(C)C1=CC2=C(C)C=CC2=C(C)C=C1  uses 2, should be 0 ?\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qedProperties\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-nightly/lib/python3.8/site-packages/rdkit/Chem/QED.py:176\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou need to provide a mol argument.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m mol \u001b[38;5;241m=\u001b[39m Chem\u001b[38;5;241m.\u001b[39mRemoveHs(mol)\n\u001b[1;32m    166\u001b[0m qedProperties \u001b[38;5;241m=\u001b[39m QEDproperties(\n\u001b[1;32m    167\u001b[0m   MW\u001b[38;5;241m=\u001b[39mrdmd\u001b[38;5;241m.\u001b[39m_CalcMolWt(mol),\n\u001b[1;32m    168\u001b[0m   ALOGP\u001b[38;5;241m=\u001b[39mCrippen\u001b[38;5;241m.\u001b[39mMolLogP(mol),\n\u001b[1;32m    169\u001b[0m   HBA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mlen\u001b[39m(mol\u001b[38;5;241m.\u001b[39mGetSubstructMatches(pattern)) \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m Acceptors\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mol\u001b[38;5;241m.\u001b[39mHasSubstructMatch(pattern)),\n\u001b[1;32m    172\u001b[0m   HBD\u001b[38;5;241m=\u001b[39mrdmd\u001b[38;5;241m.\u001b[39mCalcNumHBD(mol),\n\u001b[1;32m    173\u001b[0m   PSA\u001b[38;5;241m=\u001b[39mMolSurf\u001b[38;5;241m.\u001b[39mTPSA(mol),\n\u001b[1;32m    174\u001b[0m   ROTB\u001b[38;5;241m=\u001b[39mrdmd\u001b[38;5;241m.\u001b[39mCalcNumRotatableBonds(mol, rdmd\u001b[38;5;241m.\u001b[39mNumRotatableBondsOptions\u001b[38;5;241m.\u001b[39mStrict),\n\u001b[1;32m    175\u001b[0m   AROM\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(Chem\u001b[38;5;241m.\u001b[39mGetSSSR(Chem\u001b[38;5;241m.\u001b[39mDeleteSubstructs(Chem\u001b[38;5;241m.\u001b[39mMol(mol), AliphaticRings))),\n\u001b[0;32m--> 176\u001b[0m   ALERTS\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m alert \u001b[38;5;129;01min\u001b[39;00m StructuralAlerts \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHasSubstructMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43malert\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m    177\u001b[0m )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# The replacement\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# AROM=Lipinski.NumAromaticRings(mol),\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# is not identical. The expression above tends to count more rings\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# N1C2=CC=CC=C2SC3=C1C=CC4=C3C=CC=C4\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# OC1=C(O)C=C2C(=C1)OC3=CC(=O)C(=CC3=C2C4=CC=CC=C4)O\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# CC(C)C1=CC2=C(C)C=CC2=C(C)C=C1  uses 2, should be 0 ?\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qedProperties\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def preprocess_smiles(input_file, output_file):\n",
    "    # Read the original SMILES file\n",
    "    smiles_data = pd.read_csv(input_file, usecols=[0])\n",
    "    \n",
    "    # Prepare lists to store valid SMILES, LogP, and QED\n",
    "    valid_smiles = []\n",
    "    logp_list = []\n",
    "    qed_list = []\n",
    "    total_smiles = len(smiles_data)  # Total number of SMILES in the input file\n",
    "\n",
    "    for sm in tqdm(smiles_data.iloc[:, 0].dropna().tolist(), desc=\"Processing SMILES\"):\n",
    "        mol = Chem.MolFromSmiles(sm)\n",
    "        if mol:  # Check if the molecule is valid\n",
    "            valid_smiles.append(sm)\n",
    "            logp = Descriptors.MolLogP(mol)\n",
    "            qed = QED.qed(mol)\n",
    "            logp_list.append(logp)\n",
    "            qed_list.append(qed)\n",
    "    \n",
    "    # Create a DataFrame with the valid SMILES and their properties\n",
    "    processed_data = pd.DataFrame({\n",
    "        'SMILES': valid_smiles,\n",
    "        'LogP': logp_list,\n",
    "        'QED': qed_list\n",
    "    })\n",
    "    \n",
    "    # Save the new data to a CSV file\n",
    "    processed_data.to_csv(output_file, index=False)\n",
    "    print(f\"Preprocessed data saved to {output_file}\")\n",
    "    \n",
    "    # Calculate and print the percentage of valid SMILES\n",
    "    valid_percentage = (len(valid_smiles) / total_smiles) * 100\n",
    "    print(f\"Percentage of valid SMILES: {valid_percentage:.2f}%\")\n",
    "\n",
    "# Example usage\n",
    "preprocess_smiles('smiles_CHEMBL_22', 'processed_smiles_data_kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e755aec5-0588-4060-948d-117128cc40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smiles_data(torch.utils.data.Dataset):\n",
    "    def __init__(self, file, total=130):\n",
    "        self.total = total\n",
    "        \n",
    "        # Load the preprocessed dataset with SMILES and target variables\n",
    "        data = pd.read_csv(file).iloc[:self.total]\n",
    "        \n",
    "        self.smiles = data['SMILES'].tolist()\n",
    "        self.logp_qed = data[['LogP', 'QED']].values.tolist()\n",
    "        \n",
    "        tokens = functools.reduce(lambda acc, s: acc.union(set(s)), self.smiles, set())\n",
    "        self.vocsize = len(tokens) + len(__special__)\n",
    "        self.index2token = dict(enumerate(tokens, start=3))\n",
    "        self.index2token.update(__special__)\n",
    "        self.token2index = {v: k for k, v in self.index2token.items()}\n",
    "        \n",
    "        self.ints = [torch.LongTensor([self.token2index[s] for s in line]) for line in tqdm(self.smiles, \"Preparing the dataset\")]\n",
    "\n",
    "    def decode(self, indexes):\n",
    "        return \"\".join([self.index2token[index] for index in indexes if index not in __special__])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        special_added = torch.cat((\n",
    "            torch.LongTensor([self.token2index['<BOS>']]),\n",
    "            self.ints[i],\n",
    "            torch.LongTensor([self.token2index['<EOS>']]),\n",
    "        ), dim=0)\n",
    "        \n",
    "        logp, qed = self.logp_qed[i]  # Get the LogP and QED values for the current index\n",
    "        return one_hot(special_added, self.vocsize).float(), special_added, torch.tensor([logp, qed], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "\n",
    "def pad_collate_fn(batch):\n",
    "    seq_len = max([item[1].size(0) for item in batch])\n",
    "    padded_batch = []\n",
    "    properties = []\n",
    "\n",
    "    for one_hot_seq, special_added, props in batch:\n",
    "        pad_length = seq_len - special_added.size(0)\n",
    "        padded_special_added = torch.cat([\n",
    "            special_added,\n",
    "            torch.LongTensor([0] * pad_length)\n",
    "        ])\n",
    "        padded_one_hot_seq = one_hot(padded_special_added, one_hot_seq.size(1)).float()\n",
    "        padded_batch.append((padded_one_hot_seq, padded_special_added))\n",
    "        properties.append(props)\n",
    "    \n",
    "    one_hot_tensors = torch.stack([item[0] for item in padded_batch])\n",
    "    special_added_tensors = torch.stack([item[1] for item in padded_batch])\n",
    "    properties = torch.stack(properties)\n",
    "    \n",
    "    return one_hot_tensors, special_added_tensors, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c2acfe-42b6-45f3-b6be-172de1765e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, latent_dim=20, hidden_size=64):\n",
    "        super(SmilesVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_lstm = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_lstm = nn.LSTM(latent_dim, int(hidden_size/2), batch_first=True)\n",
    "        self.fc_out = nn.Linear(int(hidden_size/2), vocab_size)\n",
    "        \n",
    "        # Predictor for LogP and QED (2 outputs)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2)  # 2 outputs for LogP and QED\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        _, (hidden, _) = self.encoder_lstm(x)  # Learns hidden representation of the input vector x\n",
    "        mu = self.fc_mu(hidden[-1])  # Use hidden representation to determine the center of the latent distribution (mean)\n",
    "        logvar = self.fc_logvar(hidden[-1])  # Use hidden representation to determine the spread of the latent distribution (variance)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):  # To sample from the distribution and ensure differentiability, use the reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * (std + 1e-10)\n",
    "        return z  # z is the sampled latent vector\n",
    "    \n",
    "    def decode(self, z, seq_len=10, tokenizer=None, temp=1.0):\n",
    "        batch_size = z.size(0)\n",
    "        generated_seq = torch.full((batch_size, 1), tokenizer['<BOS>'], dtype=torch.long, device=z.device)\n",
    "        hidden_state = None\n",
    "\n",
    "        logits_list = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            z_rep = z.unsqueeze(1).repeat(1, generated_seq.size(1), 1)\n",
    "            output, hidden_state = self.decoder_lstm(z_rep, hidden_state)\n",
    "            logits = self.fc_out(output[:, -1, :])  # Get logits for the last time step\n",
    "            logits_list.append(logits)\n",
    "\n",
    "            next_token_probs = torch.softmax(logits / temp, dim=-1)\n",
    "            next_token = torch.multinomial(next_token_probs, 1)\n",
    "\n",
    "            generated_seq = torch.cat((generated_seq, next_token), dim=1)\n",
    "\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "\n",
    "        # Retrieve <EOS> and <PAD> token indices from tokenizer\n",
    "        eos_token = tokenizer['<EOS>']\n",
    "        pad_token = tokenizer['<PAD>']\n",
    "\n",
    "        # Stop generation after EOS token is encountered\n",
    "        mask = torch.cumsum((generated_seq == eos_token).float(), dim=1)\n",
    "        generated_seq = generated_seq.masked_fill(mask > 0, pad_token)\n",
    "\n",
    "        return logits, generated_seq\n",
    "\n",
    "    def forward(self, x, tokenizer, seq_len=10):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        logits, _ = self.decode(z, seq_len=seq_len, tokenizer=tokenizer)\n",
    "        \n",
    "        # Predict LogP and QED\n",
    "        predicted_properties = self.predictor(z)\n",
    "        \n",
    "        return logits, mu, logvar, predicted_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7d1780c-c045-4190-a63d-63602d9d820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(reconstructed_seq, original_seq, mu, logvar, predicted_properties, true_properties, epoch, n_epochs):\n",
    "    # Flatten the tensors for CrossEntropyLoss\n",
    "    flat_reconstructed_seq = reconstructed_seq.view(-1, reconstructed_seq.size(-1))  # (batch_size * seq_len, vocab_size)\n",
    "    flat_original_seq = original_seq.view(-1)  # (batch_size * seq_len)\n",
    "    \n",
    "    # Define a mask for the padding sequences\n",
    "    pad_mask = original_seq != 0  \n",
    "    \n",
    "    # Ensure reconstruction loss uses float logits and long target\n",
    "    reconstruction_loss = nn.CrossEntropyLoss(reduction='none')(flat_reconstructed_seq, flat_original_seq.long())\n",
    "    \n",
    "    # Construct the loss without the padding mask\n",
    "    reconstruction_loss = reconstruction_loss[pad_mask.view(-1)].mean()\n",
    "    \n",
    "    # KL Divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Predictor loss (MSE Loss)\n",
    "    property_loss = nn.MSELoss()(predicted_properties, true_properties)\n",
    "\n",
    "    # Define beta for KL annealing\n",
    "    beta = min(1.0, epoch / (n_epochs * 0.4))  # KL annealing\n",
    "\n",
    "    # Construct the total loss\n",
    "    total_loss = reconstruction_loss + (beta * kl_divergence) + property_loss\n",
    "    \n",
    "    return total_loss, reconstruction_loss.item(), kl_divergence.item(), property_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dad71be-a451-4dde-9fb8-e07530caea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing the dataset: 100%|████████| 150000/150000 [00:01<00:00, 104725.28it/s]\n",
      "Training:   0%|                              | 3/4688 [00:06<2:46:21,  2.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Example call to the modified training function\u001b[39;00m\n\u001b[1;32m     88\u001b[0m subset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150000\u001b[39m\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_smiles_22.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(train_file, batch_size, learning_rate, n_epochs, latent_dim, hidden_size, subset_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get the sequence length from the target sequence\u001b[39;00m\n\u001b[1;32m     60\u001b[0m reconstructed_seq, mu, logvar, predicted_properties \u001b[38;5;241m=\u001b[39m model(batch, dataset\u001b[38;5;241m.\u001b[39mtoken2index, seq_len\u001b[38;5;241m=\u001b[39mseq_len)\n\u001b[0;32m---> 62\u001b[0m loss, reconstruction_loss, kl_divergence, property_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvae_loss_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreconstructed_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_properties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_properties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     67\u001b[0m total_reconstruction_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reconstruction_loss\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mvae_loss_function\u001b[0;34m(reconstructed_seq, original_seq, mu, logvar, predicted_properties, true_properties, epoch, n_epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)(flat_reconstructed_seq, flat_original_seq\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Construct the loss without the padding mask\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruction_loss\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# KL Divergence\u001b[39;00m\n\u001b[1;32m     16\u001b[0m kl_divergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m logvar \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m logvar\u001b[38;5;241m.\u001b[39mexp())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_and_save_losses(epoch_reconstruction_losses, epoch_kl_divergences, epoch_property_losses, n_epochs):\n",
    "    \"\"\"\n",
    "    Plots and saves the reconstruction loss, KL divergence, and property loss across epochs.\n",
    "    \"\"\"\n",
    "    epochs = list(range(1, n_epochs + 1))\n",
    "    \n",
    "    # Plot Reconstruction Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, epoch_reconstruction_losses, label=\"Reconstruction Loss\")\n",
    "    plt.plot(epochs, epoch_kl_divergences, label=\"KL Divergence\")\n",
    "    plt.plot(epochs, epoch_property_losses, label=\"Property Loss (LogP/QED)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Losses Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(\"vae_training_losses.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save the losses to a CSV file\n",
    "    df_losses = pd.DataFrame({\n",
    "        \"Epoch\": epochs,\n",
    "        \"Reconstruction Loss\": epoch_reconstruction_losses,\n",
    "        \"KL Divergence\": epoch_kl_divergences,\n",
    "        \"Property Loss\": epoch_property_losses\n",
    "    })\n",
    "    df_losses.to_csv(\"vae_training_losses.csv\", index=False)\n",
    "    print(\"Training losses saved to vae_training_losses.csv and vae_training_losses.png\")\n",
    "\n",
    "def train_vae(train_file, batch_size=32, learning_rate=0.001, n_epochs=50, latent_dim=40, hidden_size=128, subset_size=50):\n",
    "    save_file = f\"vae_pred_subset{subset_size}_latent{latent_dim}_hidden{hidden_size}_epochs{n_epochs}.pt\"\n",
    "    \n",
    "    dataset = Smiles_data(train_file, total=subset_size)\n",
    "    \n",
    "    model = SmilesVAE(dataset.vocsize, latent_dim, hidden_size).to(device)\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epoch_reconstruction_losses = []\n",
    "    epoch_kl_divergences = []\n",
    "    epoch_property_losses = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        total_loss = 0.\n",
    "        total_reconstruction_loss = 0.\n",
    "        total_kl_divergence = 0.\n",
    "        total_property_loss = 0.\n",
    "\n",
    "        for iteration, (batch, target, true_properties) in enumerate(tqdm(dataloader, 'Training')):\n",
    "            batch, target, true_properties = batch.to(device), target.to(device), true_properties.to(device)\n",
    "            model.train()\n",
    "            seq_len = target.size(1)  # Get the sequence length from the target sequence\n",
    "            reconstructed_seq, mu, logvar, predicted_properties = model(batch, dataset.token2index, seq_len=seq_len)\n",
    "            \n",
    "            loss, reconstruction_loss, kl_divergence, property_loss = vae_loss_function(\n",
    "                reconstructed_seq, target, mu, logvar, predicted_properties, true_properties, epoch, n_epochs\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_reconstruction_loss += reconstruction_loss\n",
    "            total_kl_divergence += kl_divergence\n",
    "            total_property_loss += property_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_reconstruction_losses.append(total_reconstruction_loss / len(dataloader))\n",
    "        epoch_kl_divergences.append(total_kl_divergence / len(dataloader))\n",
    "        epoch_property_losses.append(total_property_loss / len(dataloader))\n",
    "\n",
    "        print(f\"Epoch {epoch} of {n_epochs} done, total loss: {total_loss}\")\n",
    "\n",
    "    # Plot and save the losses\n",
    "    plot_and_save_losses(epoch_reconstruction_losses, epoch_kl_divergences, epoch_property_losses, n_epochs)\n",
    "\n",
    "    torch.save({'tokenizer': dataset.index2token, 'model': model.cpu()}, f\"models/{save_file}\")\n",
    "    print(f\"VAE Training Complete! Model saved as {save_file}\")\n",
    "\n",
    "# Example call to the modified training function\n",
    "subset_size = 150000\n",
    "train_vae(train_file='processed_smiles_22.csv', batch_size=32, n_epochs=35, latent_dim=60, hidden_size=256, subset_size=subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14223728-e590-4de7-b441-b7d56158a4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid unique SMILES: CC1=cCCCO1\n",
      "Valid unique SMILES: CCc1cCCcc1\n",
      "Valid unique SMILES: CCCCCC2Cc2\n",
      "Valid unique SMILES: CC1CCcCcC1\n",
      "Valid unique SMILES: Cc1CCCCCc1\n",
      "Valid unique SMILES: CC1CNccCC1\n",
      "Valid unique SMILES: CNC1ccCcc1\n",
      "Valid unique SMILES: COCCCc1cC1\n",
      "Valid unique SMILES: CCCCC1cC1C\n",
      "Valid unique SMILES: CNc1CcCCC1\n",
      "Valid unique SMILES: COC1OCc1CC\n",
      "Generated 11 valid molecules and saved to generated_molecules.csv\n",
      "        SMILES\n",
      "0   CC1=cCCCO1\n",
      "1   CCc1cCCcc1\n",
      "2   CCCCCC2Cc2\n",
      "3   CC1CCcCcC1\n",
      "4   Cc1CCCCCc1\n",
      "5   CC1CNccCC1\n",
      "6   CNC1ccCcc1\n",
      "7   COCCCc1cC1\n",
      "8   CCCCC1cC1C\n",
      "9   CNc1CcCCC1\n",
      "10  COC1OCc1CC\n"
     ]
    }
   ],
   "source": [
    "# Function to decode token indices into SMILES\n",
    "def decode_from_indices(token_indices, tokenizer):\n",
    "    # Join tokens into a string and remove any unwanted special tokens\n",
    "    smiles = \"\".join([tokenizer.get(i, '') for i in token_indices if i not in __special__])\n",
    "    \n",
    "    # Remove any unwanted characters like tabs or newlines from the generated string\n",
    "    smiles = smiles.replace('\\t', '').replace('\\n', '').strip()\n",
    "    \n",
    "    return smiles\n",
    "\n",
    "def validate_smiles(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is not None:\n",
    "            return True\n",
    "    except:\n",
    "        print(f\"SMILES parsing failed: {smiles}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def generate_molecules(model, tokenizer, num_samples, seq_len, target_logp=None, target_qed=None, temp=1.0, max_temp=1.05, max_attempts=500):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    generated_smiles = []\n",
    "    unique_smiles = set()  # Set to track unique SMILES\n",
    "    token2index = {v: k for k, v in tokenizer.items()}  # Reverse tokenizer mapping\n",
    "    \n",
    "    try:\n",
    "        eos_token_index = token2index['<EOS>']  # Retrieve the index for the <EOS> token\n",
    "    except KeyError:\n",
    "        raise KeyError(\"The '<EOS>' token is missing in the tokenizer.\")\n",
    "    \n",
    "    initial_temp = temp  \n",
    "    sample_idx = 0\n",
    "    attempts = 0  \n",
    "    \n",
    "    while sample_idx < num_samples:\n",
    "        if attempts >= max_attempts:  \n",
    "            new_temp = min(temp * 1.01, max_temp)  # Increase temperature but cap it\n",
    "            if new_temp != temp:\n",
    "                temp = new_temp\n",
    "                print(f\"Increasing temperature to {round(temp,3)} for more diversity\")\n",
    "            attempts = 0 \n",
    "        \n",
    "        # Sample from the prior (standard normal) with applied temperature scaling\n",
    "        z = torch.randn(1, model.latent_dim) * temp  # Shape: (1, latent_dim)\n",
    "\n",
    "        # Check if we have target values to aim for\n",
    "        if target_logp is not None and target_qed is not None:\n",
    "            # Iteratively adjust z to aim for the target properties (e.g., through gradient descent)\n",
    "            z = aim_for_target_properties(model, z, target_logp, target_qed)\n",
    "        \n",
    "        # Decode the latent vector to generate logits and the token sequence\n",
    "        logits, sampled_tokens = model.decode(z, seq_len=seq_len, tokenizer=token2index, temp=temp)  # Pass token2index\n",
    "        \n",
    "        # Convert token indices back to SMILES string\n",
    "        token_indices = sampled_tokens[0].tolist()  # Take the first (and only) sample from batch\n",
    "        if eos_token_index in token_indices:\n",
    "            token_indices = token_indices[:token_indices.index(eos_token_index)]  # Stop at <EOS>\n",
    "        \n",
    "        smiles = decode_from_indices(token_indices, tokenizer)\n",
    "        \n",
    "        # Validate SMILES string\n",
    "        if validate_smiles(smiles):\n",
    "            if smiles not in unique_smiles:\n",
    "                print(f\"Valid unique SMILES: {smiles}\")\n",
    "                generated_smiles.append(smiles)\n",
    "                unique_smiles.add(smiles)\n",
    "                sample_idx += 1\n",
    "                attempts = 0  # Reset attempts counter after successful generation\n",
    "                temp = initial_temp  # Reset temperature to initial value\n",
    "            else:\n",
    "                attempts += 1  # Increment attempts\n",
    "        else:\n",
    "            attempts += 1  # Increment attempts for invalid SMILES\n",
    "    \n",
    "    return generated_smiles\n",
    "\n",
    "def aim_for_target_properties(model, z, target_logp, target_qed, lr=0.01, steps=10):\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([z], lr=lr)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_properties = model.predictor(z)\n",
    "        predicted_logp, predicted_qed = predicted_properties[0]\n",
    "\n",
    "        # Loss to guide z towards the target properties\n",
    "        loss = ((predicted_logp - target_logp) ** 2 + (predicted_qed - target_qed) ** 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return z.detach()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model and tokenizer\n",
    "    checkpoint = torch.load('models/vae_pred_subset150000_latent60_hidden256_epochs35.pt')\n",
    "    model = checkpoint['model']\n",
    "    tokenizer = checkpoint['tokenizer']\n",
    "    \n",
    "    # Define target properties\n",
    "    target_logp = 1.8\n",
    "    target_qed = 0.6\n",
    "    \n",
    "    # Generate molecules\n",
    "    num_samples = 11\n",
    "    temperature = 0.55\n",
    "    seq_len = 11\n",
    "    generated_smiles = generate_molecules(model, tokenizer, num_samples, seq_len, target_logp=target_logp, target_qed=target_qed, temp=temperature)\n",
    "\n",
    "    # Save generated SMILES to a file\n",
    "    df = pd.DataFrame(generated_smiles, columns=[\"SMILES\"])\n",
    "    df.to_csv(f\"molecules/generated_molecules_len{seq_len}.csv\", index=False)\n",
    "    \n",
    "    print(f\"Generated {len(generated_smiles)} valid molecules and saved to generated_molecules.csv\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdf565c8-295d-487e-8cf6-b5450e6f2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating seq_len = 3\n",
      "Valid unique SMILES: CC\n",
      "Valid unique SMILES: OC\n",
      "Valid unique SMILES: CO\n",
      "Valid unique SMILES: ON\n",
      "Valid unique SMILES: CN\n",
      "Valid unique SMILES: NO\n",
      "Valid unique SMILES: NC\n",
      "Valid unique SMILES: FC\n",
      "Valid unique SMILES: CS\n",
      "Valid unique SMILES: OO\n",
      "Sampling efficiency for seq_len 3: 22.73%\n",
      "Evaluating seq_len = 4\n",
      "Valid unique SMILES: CCC\n",
      "Valid unique SMILES: OCN\n",
      "Valid unique SMILES: COC\n",
      "Valid unique SMILES: OSO\n",
      "Valid unique SMILES: CNC\n",
      "Valid unique SMILES: ClC\n",
      "Valid unique SMILES: FNC\n",
      "Valid unique SMILES: SCC\n",
      "Valid unique SMILES: CNN\n",
      "Valid unique SMILES: CON\n",
      "Sampling efficiency for seq_len 4: 16.13%\n",
      "Evaluating seq_len = 5\n",
      "Valid unique SMILES: OSCC\n",
      "Valid unique SMILES: CCCC\n",
      "Valid unique SMILES: COCC\n",
      "Valid unique SMILES: ClCC\n",
      "Valid unique SMILES: ONCC\n",
      "Valid unique SMILES: CCCN\n",
      "Valid unique SMILES: CNOP\n",
      "Valid unique SMILES: NCCC\n",
      "Valid unique SMILES: NNCC\n",
      "Valid unique SMILES: CNCC\n",
      "Sampling efficiency for seq_len 5: 7.75%\n",
      "Evaluating seq_len = 6\n",
      "Valid unique SMILES: CCC=C\n",
      "Valid unique SMILES: CCCCO\n",
      "Valid unique SMILES: CONCN\n",
      "Valid unique SMILES: NN(C)\n",
      "Valid unique SMILES: CCCCC\n",
      "Valid unique SMILES: CC(C)\n",
      "Valid unique SMILES: COCCO\n",
      "Valid unique SMILES: ONN=C\n",
      "Valid unique SMILES: CO(C)\n",
      "Valid unique SMILES: NC(C)\n",
      "Sampling efficiency for seq_len 6: 3.15%\n",
      "Evaluating seq_len = 7\n",
      "Valid unique SMILES: CNCCNN\n",
      "Valid unique SMILES: CCOCNN\n",
      "Valid unique SMILES: CC(CC)\n",
      "Valid unique SMILES: OC1cc1\n",
      "Valid unique SMILES: CCCCNC\n",
      "Valid unique SMILES: OC(CO)\n",
      "Valid unique SMILES: CCCCOC\n",
      "Valid unique SMILES: COCCOO\n",
      "Valid unique SMILES: CC1CN1\n",
      "Valid unique SMILES: CC(C)C\n",
      "Sampling efficiency for seq_len 7: 1.31%\n",
      "Evaluating seq_len = 8\n",
      "Valid unique SMILES: CN1cc1C\n",
      "Valid unique SMILES: CC(C)=C\n",
      "Valid unique SMILES: CCC1Oc1\n",
      "Valid unique SMILES: CC1CcC1\n",
      "Valid unique SMILES: NC1=cC1\n",
      "Valid unique SMILES: OC1Ccc1\n",
      "Valid unique SMILES: COc1NC1\n",
      "Valid unique SMILES: CCN(CC)\n",
      "Valid unique SMILES: CSCCOOO\n",
      "Valid unique SMILES: CCC1CO1\n",
      "Sampling efficiency for seq_len 8: 0.84%\n",
      "Evaluating seq_len = 9\n",
      "Valid unique SMILES: CC1COcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CNc1ccc1\n",
      "Valid unique SMILES: CCCC1nN1\n",
      "Valid unique SMILES: CC1=ccN1\n",
      "Valid unique SMILES: CNC1cOc1\n",
      "Valid unique SMILES: OOC1Ccc1\n",
      "Valid unique SMILES: C(CNO)CC\n",
      "Valid unique SMILES: FOCCCCCS\n",
      "Valid unique SMILES: OCCCONCO\n",
      "Valid unique SMILES: CN1COsN1\n",
      "Sampling efficiency for seq_len 9: 0.44%\n",
      "Evaluating seq_len = 10\n",
      "Valid unique SMILES: CC(n1)CC1\n",
      "Valid unique SMILES: CC1cCCcC1\n",
      "Valid unique SMILES: COCCCCN\\C\n",
      "Valid unique SMILES: OC1NcOOc1\n",
      "Valid unique SMILES: OOc1NCOC1\n",
      "Valid unique SMILES: OCC[O]CNC\n",
      "Valid unique SMILES: CC(C)OCCC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC(COCC)O\n",
      "Valid unique SMILES: CC(cc1)N1\n",
      "Valid unique SMILES: CC(C)NCCN\n",
      "Sampling efficiency for seq_len 10: 0.54%\n",
      "Evaluating seq_len = 11\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC(CCl)ONC\n",
      "Valid unique SMILES: NC1CccCNc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: COC1CcCcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1COCOcc1\n",
      "Valid unique SMILES: CCC1CcCCc1\n",
      "Valid unique SMILES: CS1=cc1CCN\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCC1ccCC1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: OCC1COc1OC\n",
      "Valid unique SMILES: CCc1OOOCC1\n",
      "Valid unique SMILES: CNc1CcCN1C\n",
      "Sampling efficiency for seq_len 11: 0.20%\n",
      "Evaluating seq_len = 12\n",
      "Valid unique SMILES: NC1c=c2cc21\n",
      "Valid unique SMILES: OC1CcOOCNC1\n",
      "Valid unique SMILES: On1CNccCC1N\n",
      "Valid unique SMILES: NC1=nOCNcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: COCc1Cc1C=C\n",
      "Valid unique SMILES: FOc1CCc1CSC\n",
      "Valid unique SMILES: COC1Cc=ncc1\n",
      "Valid unique SMILES: C\\C(C)NCOCO\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1CnC1(CO)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1COCBcOC1\n",
      "Sampling efficiency for seq_len 12: 0.24%\n",
      "Evaluating seq_len = 13\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CC1ncccSC1=O\n",
      "Valid unique SMILES: CNc1cNCC=CC1\n",
      "Valid unique SMILES: O\\1\\CCCcOnc1\n",
      "Valid unique SMILES: OC1CCccCCcc1\n",
      "Valid unique SMILES: CN1CcCnCcNO1\n",
      "Valid unique SMILES: CCCCOOc1CCc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCCCC1O(cC1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CNC1cCc(cN1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1ccCC2Cc21\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CN1NCc(NN1)C\n",
      "Sampling efficiency for seq_len 13: 0.20%\n",
      "Evaluating seq_len = 14\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CC1CCc/Cc1CCS\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CCc1csccC1C=C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCc1cOnCcCcc1\n",
      "Valid unique SMILES: C\\c1c(cOc1)CC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: OCC=COON(N)NC\n",
      "Valid unique SMILES: CCc1NCccccCc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Valid unique SMILES: CN(Cc1cCC1CC)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CC1=cCC2CNc21\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: OC1=cnccOOcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Valid unique SMILES: FNCCNC1CCC1=C\n",
      "Sampling efficiency for seq_len 14: 0.05%\n",
      "Evaluating seq_len = 15\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Valid unique SMILES: CN(COc1cccNC1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCCC1COCcNSC1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: CCCCOc1CcCCcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CNCC2c/C2N(O)N\n",
      "Valid unique SMILES: CC1ccccCc1CNCO\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CSc1cc=ccCNOc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: OOc1ccccONc1CN\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Valid unique SMILES: CCc1ccccCNNc1C\n",
      "Valid unique SMILES: OOCCCCn2CCn2OC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: NC1CccC#CcCNc1\n",
      "Sampling efficiency for seq_len 15: 0.05%\n",
      "Evaluating seq_len = 16\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: FC(nOC2)\\nc2CCC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Valid unique SMILES: NN1C=C/cCc2SC12\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1CCOcCSccCc1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1ccCCC=c(C1)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CC1CcCcCCCCNcC1\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Valid unique SMILES: CN1nCCCCN1C2OC2\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CN(s=CcCc2)cCC2\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCN1cccnC=CCc1O\n",
      "Valid unique SMILES: Cc1cCCcNCcccc1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1CccCS1N(CC)\n",
      "Sampling efficiency for seq_len 16: 0.02%\n",
      "Evaluating seq_len = 17\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Valid unique SMILES: CC1COcCCCC11CN1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: C1CCCccCc1(NC=C)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: Cc(CcC(CC1))1CNC\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: OCN1N(c1C2CC)cn2\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CC(=NC(CNNN)CSC)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCCCc2CCCc1CC21N\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: COC(S1ccCCcc1CC)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCCCN1cNCCcc=c1C\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCC1cC12ON(OCc2)\n",
      "Increasing temperature to 1.01 for more diversity\n",
      "Increasing temperature to 1.02 for more diversity\n",
      "Increasing temperature to 1.03 for more diversity\n",
      "Increasing temperature to 1.041 for more diversity\n",
      "Increasing temperature to 1.05 for more diversity\n",
      "Valid unique SMILES: CCc1CcOCccSccCN1\n",
      "Sampling efficiency for seq_len 17: 0.02%\n",
      "Saved sampling efficiency results to sampling_efficiency_per_seq_len.csv\n",
      "    seq_len  sampling_efficiency\n",
      "0         3            22.727273\n",
      "1         4            16.129032\n",
      "2         5             7.751938\n",
      "3         6             3.154574\n",
      "4         7             1.310616\n",
      "5         8             0.841751\n",
      "6         9             0.443459\n",
      "7        10             0.544959\n",
      "8        11             0.204625\n",
      "9        12             0.242954\n",
      "10       13             0.199322\n",
      "11       14             0.048022\n",
      "12       15             0.048077\n",
      "13       16             0.020873\n",
      "14       17             0.016955\n"
     ]
    }
   ],
   "source": [
    "def compute_sampling_efficiency(generated_smiles, total_attempts):\n",
    "    \"\"\"\n",
    "    Computes the sampling efficiency of the model.\n",
    "\n",
    "    Sampling efficiency is defined as the ratio of valid unique SMILES strings\n",
    "    to the total number of attempts made during the sampling process.\n",
    "\n",
    "    Args:\n",
    "        generated_smiles (list): A list of valid unique SMILES strings.\n",
    "        total_attempts (int): Total number of sampling attempts.\n",
    "\n",
    "    Returns:\n",
    "        float: The sampling efficiency as a percentage.\n",
    "    \"\"\"\n",
    "    num_unique_smiles = len(generated_smiles)\n",
    "    if total_attempts == 0:\n",
    "        return 0.0\n",
    "    return (num_unique_smiles / total_attempts) * 100\n",
    "\n",
    "\n",
    "def generate_molecules_with_efficiency(model, tokenizer, num_samples, seq_len, target_logp=None, target_qed=None, temp=0.55, max_temp=1.05, max_attempts=500):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    generated_smiles = []\n",
    "    unique_smiles = set()  # Set to track unique SMILES\n",
    "    token2index = {v: k for k, v in tokenizer.items()}  # Reverse tokenizer mapping\n",
    "    total_attempts = 0  # Track the total number of attempts\n",
    "\n",
    "    try:\n",
    "        eos_token_index = token2index['<EOS>']  # Retrieve the index for the <EOS> token\n",
    "    except KeyError:\n",
    "        raise KeyError(\"The '<EOS>' token is missing in the tokenizer.\")\n",
    "    \n",
    "    initial_temp = temp  \n",
    "    sample_idx = 0\n",
    "    attempts = 0  \n",
    "    \n",
    "    while sample_idx < num_samples:\n",
    "        if attempts >= max_attempts:  \n",
    "            new_temp = min(temp * 1.01, max_temp)  # Increase temperature but cap it\n",
    "            if new_temp != temp:\n",
    "                temp = new_temp\n",
    "                print(f\"Increasing temperature to {round(temp,3)} for more diversity\")\n",
    "            attempts = 0 \n",
    "        \n",
    "        # Sample from the prior (standard normal) with applied temperature scaling\n",
    "        z = torch.randn(1, model.latent_dim) * temp  # Shape: (1, latent_dim)\n",
    "\n",
    "        # Check if we have target values to aim for\n",
    "        if target_logp is not None and target_qed is not None:\n",
    "            # Iteratively adjust z to aim for the target properties (e.g., through gradient descent)\n",
    "            z = aim_for_target_properties(model, z, target_logp, target_qed)\n",
    "        \n",
    "        # Decode the latent vector to generate logits and the token sequence\n",
    "        logits, sampled_tokens = model.decode(z, seq_len=seq_len, tokenizer=token2index, temp=temp)  # Pass token2index\n",
    "        \n",
    "        # Convert token indices back to SMILES string\n",
    "        token_indices = sampled_tokens[0].tolist()  # Take the first (and only) sample from batch\n",
    "        if eos_token_index in token_indices:\n",
    "            token_indices = token_indices[:token_indices.index(eos_token_index)]  # Stop at <EOS>\n",
    "        \n",
    "        smiles = decode_from_indices(token_indices, tokenizer)\n",
    "        \n",
    "        # Validate SMILES string\n",
    "        if validate_smiles(smiles):\n",
    "            if smiles not in unique_smiles:\n",
    "                print(f\"Valid unique SMILES: {smiles}\")\n",
    "                generated_smiles.append(smiles)\n",
    "                unique_smiles.add(smiles)\n",
    "                sample_idx += 1\n",
    "                attempts = 0  # Reset attempts counter after successful generation\n",
    "                temp = initial_temp  # Reset temperature to initial value\n",
    "            else:\n",
    "                attempts += 1  # Increment attempts\n",
    "        else:\n",
    "            attempts += 1  # Increment attempts for invalid SMILES\n",
    "        \n",
    "        total_attempts += 1  # Count every attempt\n",
    "    \n",
    "    sampling_efficiency = compute_sampling_efficiency(generated_smiles, total_attempts)\n",
    "    # Save generated SMILES to a file\n",
    "    df = pd.DataFrame(generated_smiles, columns=[\"SMILES\"])\n",
    "    df.to_csv(f\"molecules/generated_molecules_len{seq_len}.csv\", index=False)\n",
    "    print(f\"Sampling efficiency for seq_len {seq_len}: {sampling_efficiency:.2f}%\")\n",
    "\n",
    "    return generated_smiles, sampling_efficiency\n",
    "\n",
    "\n",
    "def evaluate_sampling_efficiency_across_seq_lengths(model, tokenizer, num_samples_per_seq_len, seq_len_range, target_logp=None, target_qed=None, temp=1.0):\n",
    "    \"\"\"\n",
    "    Evaluates sampling efficiency across a range of sequence lengths and saves the results.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to generate molecules.\n",
    "        tokenizer: Tokenizer for converting indices to SMILES strings.\n",
    "        num_samples_per_seq_len (int): Number of samples to generate per sequence length.\n",
    "        seq_len_range (list): A list of sequence lengths to evaluate.\n",
    "        target_logp: Optional target logP value for property-based generation.\n",
    "        target_qed: Optional target QED value for property-based generation.\n",
    "        temp (float): Initial temperature for sampling.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing seq_len and sampling efficiency values.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for seq_len in seq_len_range:\n",
    "        print(f\"Evaluating seq_len = {seq_len}\")\n",
    "        _, sampling_efficiency = generate_molecules_with_efficiency(\n",
    "            model, tokenizer, num_samples_per_seq_len, seq_len, target_logp=target_logp, target_qed=target_qed, temp=temp\n",
    "        )\n",
    "        results.append({'seq_len': seq_len, 'sampling_efficiency': sampling_efficiency})\n",
    "    \n",
    "    # Convert results to DataFrame for easy saving\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(\"sampling_efficiency_per_seq_len.csv\", index=False)\n",
    "    print(\"Saved sampling efficiency results to sampling_efficiency_per_seq_len.csv\")\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the trained model and tokenizer\n",
    "    checkpoint = torch.load('models/vae_pred_subset150000_latent100_hidden256_epochs8.pt')\n",
    "    model = checkpoint['model']\n",
    "    tokenizer = checkpoint['tokenizer']\n",
    "    \n",
    "    # Define target properties (optional)\n",
    "    target_logp = 2.0\n",
    "    target_qed = 0.6\n",
    "    \n",
    "    # Evaluate sampling efficiency across a range of sequence lengths\n",
    "    seq_len_range = range(3, 18, 1)  # Sequence lengths from 10 to 50, in steps of 5\n",
    "    num_samples_per_seq_len = 10  # Generate 10 samples for each sequence length\n",
    "    \n",
    "    df_efficiency = evaluate_sampling_efficiency_across_seq_lengths(\n",
    "        model, tokenizer, num_samples_per_seq_len, seq_len_range, target_logp=target_logp, target_qed=target_qed\n",
    "    )\n",
    "\n",
    "    print(df_efficiency)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
